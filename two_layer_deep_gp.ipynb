{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "two_layer_deep_gp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNTUBwk1+o5gRbbgk3l9vvc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svetaU/DGP/blob/main/two_layer_deep_gp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An implementation of deep gaussian process with two layers using hamiltonian monte carlo sampling of the full posterior. Inspired by papers and online articles: https://doi.org/10.48550/arxiv.2012.08015 ; https://colindcarroll.com/2019/04/11/hamiltonian-monte-carlo-from-scratch/ ; https://bayesianbrad.github.io/posts/2019_hmc.html"
      ],
      "metadata": {
        "id": "xALZV9Zo6h1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "6suuduHq8zdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import scipy.stats as st\n",
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation, writers\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "4YwUzIP922O1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd587b3b-554b-456f-e310-150e7e380625"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount colab drives for data"
      ],
      "metadata": {
        "id": "YH48y0em83Ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, subprocess\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "  print(\"You are in Google Colab environment!\")\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "else:\n",
        "  print(\"This notebook is best run in Google Colab.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt5P857n875U",
        "outputId": "8775b228-6adc-4c17-9f4c-9921f90545b1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are in Google Colab environment!\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HMC"
      ],
      "metadata": {
        "id": "WlWqL6nuQIql"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "R76KAy11z_VV"
      },
      "outputs": [],
      "source": [
        "def leapfrog(q, p, dUdq, num_steps, step_size, random_step=False,m=None):\n",
        "  if random_step:\n",
        "    step_size = np.random.uniform(step_size*0.8,step_size*1.2,1)\n",
        "  if m is None:\n",
        "    m = np.ones(len(p))\n",
        "  q, p = np.copy(q), np.copy(p)\n",
        "  p -= step_size * dUdq(q) / 2.0  \n",
        "  leap_path = [np.copy(q)]\n",
        "  for _ in range(num_steps - 1):\n",
        "      q += step_size * p/m  \n",
        "      p -= step_size * dUdq(q) \n",
        "      leap_path.append(np.copy(q))\n",
        "  q += step_size * p/m  \n",
        "  p -= step_size * dUdq(q) / 2.0\n",
        "  leap_path.append(np.copy(q)) \n",
        "  return (q,-p,leap_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def hamiltonian_monte_carlo(U, grad_U, current_position, steps=1, delta_t=0.5, change_step = False, masses = None):\n",
        "  q = np.copy(current_position)\n",
        "  p = np.random.normal(0.0,1.0,len(q)) \n",
        "  current_momentum = np.copy(p)\n",
        "  if masses is None:\n",
        "    masses = np.ones(len(p))\n",
        "  q, p, path = leapfrog(q,p,grad_U,steps,delta_t,change_step, masses)\n",
        "  current_U = U(current_position)\n",
        "  current_K = sum(current_momentum*current_momentum/masses) / 2\n",
        "  proposed_U = U(q)\n",
        "  proposed_K = sum(p*p/masses) / 2\n",
        "  if (np.log(np.random.rand()) < (current_U-proposed_U+current_K-proposed_K)):\n",
        "    return (q,proposed_U,1,path) # accept\n",
        "  else:\n",
        "    return (current_position, current_U,0,None) # reject"
      ],
      "metadata": {
        "id": "BYGtoap__aD0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Posterior for Gaussian Process (GP)\n",
        "The model ($Y_n$ is observed GP, $W_n$ is hidden layer GP of the same dim) \n",
        "$$ \n",
        "Y_n | W âˆ¼ \\mathcal{N}_n(0,\\tau^2(K_{\\theta_y}(W) +g\\mathbb{I}_n)),\\;\\; W_n \\sim \\mathcal{N}(0,K_{\\theta_w}(X_n)),\\;\\; K_{\\theta}^{ij} = e^{-\\frac{||x_i-x_j||^2}{\\theta}}\n",
        "$$\n",
        "\n",
        "With thus defined probabilities, the log likelihoods are (|A| is determinate of a matrix A)\n",
        "\n",
        "$$\n",
        "log \\mathcal{L}(Y_n|W,\\theta_y,g) \\propto - \\frac{n}{2}log(n\\hat{\\tau}^2)-\\frac{1}{2}log|K_{\\theta_y}(W)+g\\mathbb{I}_n|\n",
        "$$\n",
        " \n",
        "$$\n",
        "log \\mathcal{L}(W|X,\\theta_w) \\propto -\\frac{1}{2}log|K_{\\theta_w}(X)| - \\frac{1}{2} W^\\top K^{-1}_{\\theta_w}(X)W\n",
        "$$\n",
        "\n",
        "with $\\hat{\\tau}^2$ is defined as a point estimate (not sampled by MCMC)\n",
        "$$\n",
        "\\hat{\\tau}^2=\\frac{Y^\\top(K_{\\theta_y}(W) + g\\mathbb{I}_n)^{-1}Y}{n}\n",
        "$$\n",
        "via MLE with reference prior ($1/\\tau^2$).\n",
        "\n",
        "Priors for other hyperparameters are gamma distributions.\n",
        "\n",
        "Altogether, up to a constant, the posterior for the model params looks like\n",
        "$$\n",
        "-log(\\pi(W,\\vec{\\theta},g|D_n)) = -log\\mathcal{L}(Y_n|W,\\theta_y,g) - log \\mathcal{L}(W|X_n,\\theta_w) - (a_{\\theta_y}-1)log(\\theta_y)+b_{\\theta_y}\\theta_y - (a_{\\theta_w}-1)log(\\theta_w)+b_{\\theta_w}\\theta_w - (a_{g}-1)log(g)+b_{g}g\n",
        "$$\n",
        "\n",
        "with $D_n$ being the data $(Y_n,X_n)$. Here $a, b$ enter gamma pdf as $p(x;a,b) = \\frac{x^{a-1}e^{-bx}b^a}{\\Gamma(a)}$"
      ],
      "metadata": {
        "id": "uwCOu6nrQB5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TLdeepGP():\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "    self.data = ([],[])\n",
        "    self.a_ty =1.1\n",
        "    self.b_ty = 1.1\n",
        "    self.a_tw =1.1\n",
        "    self.b_tw = 1.1\n",
        "    self.a_g =1.1\n",
        "    self.b_g = 1.1\n",
        "\n",
        "  def set_data(self,data):\n",
        "    self.data = np.copy(data)\n",
        "    self.x = self.data[0]\n",
        "    self.y = self.data[1]\n",
        "    self.y = self.y - np.mean(self.y)\n",
        "    self.n = len(self.y)\n",
        "\n",
        "  def log_likelihood(self,params): # -log(p)\n",
        "  # We should have total (3 + number of points) params that go like (g,theta_w,theta_y,W)\n",
        "    noise_matrix = params[0]*tf.eye(num_rows = self.n,dtype=tf.float32)\n",
        "    det_noise = tf.linalg.det(noise_matrix)\n",
        "    inv_noise = tfp.math.lu_matrix_inverse(*tf.linalg.lu(noise_matrix))\n",
        "    x = tf.convert_to_tensor(params[3:], dtype=tf.float32)\n",
        "    x_row = tf.expand_dims(x, axis=1)\n",
        "    x_col = tf.expand_dims(x, axis=0)\n",
        "    dist = tf.maximum(0.,tf.pow(x_row,2) + tf.pow(x_col,2) - 2.*tf.matmul(x_row,x_col))\n",
        "    print(x)\n",
        "    print(dist)\n",
        "    t = 0\n",
        "    return t   \n",
        "\n",
        "  def tf_log_likelihood(self,params): # -log(p)\n",
        "    y = 0\n",
        "    return y \n",
        "\n",
        "  def log_priors(self,params): # -log(\\pi)\n",
        "    p_params = - (self.a_g - 1.0)*math.log(params[0]) + self.b_g*params[0] \\\n",
        "    - (self.a_tw - 1.0)*math.log(params[1]) + self.b_tw*params[1] \\\n",
        "    - (self.a_ty - 1.0)*math.log(params[2]) + self.b_yt*params[2] \n",
        "    return p_params\n",
        "\n",
        "  def log_likelihood_grad(self,params): \n",
        "    #params = tf.convert_to_tensor(params, dtype=tf.float32)\n",
        "    #with tf.GradientTape() as tape:\n",
        "    #  tape.watch(params)\n",
        "    #  y = self.tf_log_likelihood(params)\n",
        "    #return tape.gradient(y,params).numpy()\n",
        "    return 0\n",
        "\n",
        "  def log_priors_grad(self, params):\n",
        "    return 0\n",
        "\n",
        "  def log_prob(self, params):\n",
        "    return self.log_likelihood(params) + self.log_priors(params)\n",
        "\n",
        "  def log_prob_grad(self,params):\n",
        "    return self.log_likelihood_grad(params) + self.log_priors_grad(params)\n"
      ],
      "metadata": {
        "id": "RZt78QWMbTFH"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit parameters"
      ],
      "metadata": {
        "id": "gK_lj5M2UPym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_dgp_w_hmc():\n",
        "  target_mean = [2.,3.]\n",
        "  target_cov = [[2.5,0.4],[0.4,1.6]] \n",
        "  sigma1 = round(math.sqrt(target_cov[0][0]),2)\n",
        "  sigma2 = round(math.sqrt(target_cov[1][1]),2)\n",
        "  rho = round(target_cov[0][1]/sigma1/sigma2,2)\n",
        "  sample_size = 5\n",
        "  data_sample_x, data_sample_y = np.random.multivariate_normal(target_mean,target_cov,size=sample_size).T\n",
        "  model_fit = TLdeepGP()\n",
        "  model_fit.set_data((data_sample_x,data_sample_y))\n",
        "  param_init = [0.1,1.,2.,1.,2.,3.,4.,5.]\n",
        "  model_fit.log_likelihood(param_init)"
      ],
      "metadata": {
        "id": "SlQoXwUFoPH9"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim_dots = fit_dgp_w_hmc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48ccAKj12zP4",
        "outputId": "29b3ee56-bd00-49f4-db85-e455971e0bf0"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([1. 2. 3. 4. 5.], shape=(5,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 0.  1.  4.  9. 16.]\n",
            " [ 1.  0.  1.  4.  9.]\n",
            " [ 4.  1.  0.  1.  4.]\n",
            " [ 9.  4.  1.  0.  1.]\n",
            " [16.  9.  4.  1.  0.]], shape=(5, 5), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Infer data"
      ],
      "metadata": {
        "id": "WYdhqtXqlubz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XYeWXp_EltLt"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}